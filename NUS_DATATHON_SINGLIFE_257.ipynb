{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install pyarrow\n",
    "#%pip install numpy\n",
    "#%pip install scikit-learn\n",
    "#%pip install imbalanced-learn\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/catB_train.parquet'\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #to get the size of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #to understand the datatypes of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() #find out the number of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates() #drop duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the dataframe, we will change all NAn values to 0 for f_purchase_lh and f_ever_declined_la, to indicate that clients will not purchase in the next 3 months. This is because we are basing it on the extreme and worse case scenario so that we will not overestimate our predictions for target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column_indices = list(range(1, 35)) + [-1]\n",
    "\n",
    "# Create a new DataFrame with selected columns using iloc\n",
    "df_clean = df.iloc[:, selected_column_indices]\n",
    "df_clean.loc[:, \"f_purchase_lh\"] = df_clean.loc[:, \"f_purchase_lh\"].fillna(0)\n",
    "df_clean.loc[:, \"f_ever_declined_la\"] = df_clean.loc[:, \"f_ever_declined_la\"].fillna(0)\n",
    "df_clean.drop(columns=[\"hh_20\",\"pop_20\"],inplace=True)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_clean.columns.values) #this code is just for us to check which columns are left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data_types = df_clean.dtypes\n",
    "print(column_data_types) #checking each columns' datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_clean['client_risk'] = np.where(df_clean['flg_substandard'] == 1, 'Substandard',\n",
    "                                np.where(df_clean['flg_is_borderline_standard'] == 1, 'Borderline Standard', 'Standard'))\n",
    "\n",
    "\n",
    "# We combined flg_substandard, flg_is_borderline_standard into a new column client_risk, to label the types of each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(columns=[\"flg_substandard\",\"flg_is_borderline_standard\"]) #remove the columns \"flg_substandard\" and \"flg_is_borderline_standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to help us understand size of each client type\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "client_risk_counts = df_clean['client_risk'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=client_risk_counts.index, y=client_risk_counts.values, palette='viridis')\n",
    "plt.title('Count of Each Client Risk Type')\n",
    "plt.xlabel('Client Risk Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we understand that standard client risk types make up the largest group of customer base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is to create 3 piecharts to visualise the percentage of each type of clients who will purchase insurance plan in the next 3 months. This will let us know the behaviours of each client type and how our current outreach and interactions with them are going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_risk_types = ['Substandard', 'Borderline Standard', 'Standard']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Iterate over client risk types\n",
    "for i, risk_type in enumerate(client_risk_types):\n",
    "    # Filter the DataFrame to include only rows with the current client risk type\n",
    "    df_risk_type = df_clean[df_clean['client_risk'] == risk_type]\n",
    "\n",
    "    # Replace NaN values with 0 in the 'f_purchase_lh' column\n",
    "    df_risk_type['f_purchase_lh'].fillna(0, inplace=True)\n",
    "\n",
    "    # Calculate the percentage of people who made a purchase within the subset\n",
    "    percentage_purchase = (df_risk_type['f_purchase_lh'].sum() / len(df_risk_type)) * 100\n",
    "\n",
    "    # Create a pie chart for the current client risk type\n",
    "    labels = ['Purchased', 'Not Purchased']\n",
    "    sizes = [percentage_purchase, 100 - percentage_purchase]\n",
    "    colors = ['#66b3ff', '#99ff99']  # Blue for Purchased, Green for Not Purchased\n",
    "\n",
    "    axes[i].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    axes[i].set_title(f'Client Risk Type: {risk_type}')\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle('Percentage of Purchases Based on Client Risk Type')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call',\n",
    "                         'is_consent_to_sms']\n",
    "\n",
    "# Melt the DataFrame to create a long-form representation\n",
    "df_melted = pd.melt(df[columns_of_interest], var_name='Consent Type', value_name='Consent Value')\n",
    "\n",
    "# Plot a side-by-side countplot\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.countplot(x='Consent Type', hue='Consent Value', data=df_melted, palette='Set2')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Consent Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Counts of Consent Types (0 and 1)')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Consent Value', loc='upper right', labels=['Not Granted', 'Granted'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This barchart shows an overview of the various types of communication that clients prefer. In this case, email is the most preferred form of communication. By extension, Singlife could choose to focus their marketing strategies and communication on insurance products to clients via email. On the other hand, the highest not granted form of communication is through call. Perhaps, Singlife can choose to remove call as a form as communication, since it may not be effective in reaching out to clients. This could also allow Singlife to achieve cost savings by reducing manpower in that aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_types = ['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Iterate over communication types\n",
    "for i, comm_type in enumerate(communication_types):\n",
    "    # Filter the DataFrame to include only rows where the current communication type is 1\n",
    "    df_comm_type = df_clean[df_clean[comm_type] == 1]\n",
    "\n",
    "    # Calculate the percentage of people who made a purchase within the subset\n",
    "    percentage_purchase = (df_comm_type['f_purchase_lh'].sum() / len(df_comm_type)) * 100\n",
    "\n",
    "    # Create a pie chart for the current communication type\n",
    "    labels = ['Purchased', 'Not Purchased']\n",
    "    sizes = [percentage_purchase, 100 - percentage_purchase]\n",
    "    colors = ['#66b3ff', '#99ff99']  # Blue for Purchased, Green for Not Purchased\n",
    "\n",
    "    axes[i].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    axes[i].set_title(f'Communication Type: {comm_type}')\n",
    "\n",
    "# Set overall title\n",
    "fig.suptitle('Percentage of People Who Agreed to Different Communication Types and Purchased')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piecharts show the proportion of clients who will purchase health insurance products within 3 months for each mode of communication. Clients who gave consent to communication via email have the highest percentage of choosing to purchase the health insurance products. As seen, communication via email may be the most effective way to influence clients to purchase health insurance products. On the otherhand, communication via call witnessed the lowest percentage. Just like the previous graph, communication via call may not be the most optimal mode of communication for Singlife and clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of df_clean to remove all rows that contain at least one NAn value. This is because our values are either 0 or 1 (binary) so finding the mean/median for each value is not appropriate.\n",
    "df_tree = df_clean.copy()\n",
    "df_tree = df_tree.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes below is to make a decision tree to find out which of the 3 client risk type (standard, substandard and borderline standard) will be a key indicator of whether a client purchases within the next 3 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim']\n",
    "target = 'f_purchase_lh'\n",
    "data = df_tree[features + [target]]\n",
    "\n",
    "# Split the data into features (X) and target variable (y) to get training and test data\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing a Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(decision_tree_classifier, filled=True, feature_names=features, class_names=[str(x) for x in decision_tree_classifier.classes_])\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# Display the decision tree rules\n",
    "tree_rules = export_text(decision_tree_classifier, feature_names=features)\n",
    "print(\"Decision Tree Rules:\\n\", tree_rules)\n",
    "\n",
    "# Evaluate the model on the test set (you might want to use more comprehensive evaluation metrics)\n",
    "accuracy = decision_tree_classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the decision tree above, some key points that may result are:\n",
    "Client having general claims have the largest role in predicting client NOT purchasing the policy, followed by health claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the data types of all columns in df_clean\n",
    "dtypes = df_clean.dtypes\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = dtypes[dtypes != \"object\"].index.tolist()\n",
    "\n",
    "# Create a new DataFrame with only numeric columns\n",
    "df_stepwise = df_clean[numeric_columns]\n",
    "df_stepwise = df_stepwise.dropna()\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(\"Shape of df_stepwise:\", df_stepwise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe called df_stepwise for stepwise model selection to :choose a subset of predictor variables that optimally contribute to the predictive performance of the model. These new found predictors will be used to make a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_stepwise = df_stepwise.dropna()\n",
    "# Define the dependent and independent variables\n",
    "y = df_stepwise[\"f_purchase_lh\"]\n",
    "X = df_stepwise.drop(\"f_purchase_lh\", axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LogisticRegression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the RFE object with the desired number of features\n",
    "rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE object to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the mask of selected features\n",
    "selected_features_mask = rfe.support_\n",
    "\n",
    "# Get the names of selected features\n",
    "selected_features = X_train.columns[selected_features_mask]\n",
    "\n",
    "# Print the names of selected features\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Create a new X_train with only selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "\n",
    "# Train a LogisticRegression model with selected features\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test[selected_features])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, our selected features are: flg_gi_claim, flg_is_proposal, is_valid_dm, is_valid_email,  is_housewife_retiree.\n",
    "We then make a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the dependent and independent variables\n",
    "y = df_stepwise[\"f_purchase_lh\"]\n",
    "X = df_stepwise[['flg_gi_claim', 'flg_is_proposal', 'is_valid_dm', 'is_valid_email',\n",
    "       'is_housewife_retiree']]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Create the logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logistic regression model suggests that variables such as flg_gi_claim, flg_is_proposal, is_valid_dm, is_valid_email, and is_housewife_retiree are statistically significant predictors of the likelihood of the event f_purchase_lh, and thus critical touchpoints for SingLife.\n",
    "\n",
    "The value of the log-likelihood function = 0.173166. In logistic regression, the likelihood function represents the probability of observing the given data under the estimated model. Lower values indicate a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "From the above, we know critical touchpoint clients are those who are housewives or retired.\n",
    "\n",
    "SingLife should also pay more attention to ensuring clients get their general insurance claims, as well as pushing to curate insurance proposal for clients. The best mode of communication for doing these would likely be email, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the data\n",
    "y_pred = results.predict(X)\n",
    "\n",
    "# Convert the predictions to binary values\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y, y_pred_binary)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    # Assuming df_tree is your DataFrame\n",
    "    features = ['flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim']\n",
    "    target = 'f_purchase_lh'\n",
    "\n",
    "    # Extract relevant features and target for the Decision Tree\n",
    "    data = df_tree[features + [target]]\n",
    "\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Decision Tree classifier\n",
    "    decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "    decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Visualize the decision tree\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(decision_tree_classifier, filled=True, feature_names=features, class_names=[str(x) for x in decision_tree_classifier.classes_])\n",
    "    plt.title(\"Decision Tree Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "    # Display the decision tree rules\n",
    "    tree_rules = export_text(decision_tree_classifier, feature_names=features)\n",
    "    print(\"Decision Tree Rules:\\n\", tree_rules)\n",
    "\n",
    "    # Predict missing values in the hidden data\n",
    "    hidden_data_x = hidden_data[features]\n",
    "    hidden_data_y_predicted = decision_tree_classifier.predict(hidden_data_x)\n",
    "\n",
    "    return hidden_data_y_predicted.tolist()\n",
    "\n",
    "# This cell should output a list of predicted binary classes.\n",
    "\n",
    "hidden_data = pd.read_parquet(filepath)  # Load your hidden data\n",
    "print(testing_hidden_data(hidden_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
